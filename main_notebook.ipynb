{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from probatus.feature_elimination import ShapRFECV\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_preprocess_data(file_path, drop_columns=None, date_column=None):\n",
    "    \"\"\"\n",
    "    Загружает данные из файла CSV, удаляет указанные колонки, обрабатывает дату и кодирует категориальные данные.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Загружаем данные\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Удаляем указанные колонки\n",
    "    if drop_columns:\n",
    "        df.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Обрабатываем колонку с датами, если она указана\n",
    "    if date_column:\n",
    "        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')  # Преобразуем дату, пропуская некорректные значения\n",
    "        df['year'] = df[date_column].dt.year\n",
    "        df['month'] = df[date_column].dt.month\n",
    "        df['day'] = df[date_column].dt.day\n",
    "        df['weekday'] = df[date_column].dt.weekday\n",
    "        df['quarter'] = df[date_column].dt.quarter\n",
    "        df.drop(columns=[date_column], inplace=True)\n",
    "    \n",
    "    # Заполняем пропущенные значения\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Преобразуем категориальные данные в числовые (One-Hot Encoding)\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    \n",
    "    # Преобразуем логические значения в целочисленные\n",
    "    bool_columns = df.select_dtypes(include=['bool']).columns\n",
    "    if not bool_columns.empty:\n",
    "        df[bool_columns] = df[bool_columns].astype('int')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from probatus.feature_elimination import ShapRFECV\n",
    "\n",
    "def feature_selection(X, y, n_features=5):\n",
    "    \"\"\"\n",
    "    Отбор признаков с использованием ShapRFECV..\n",
    "    \"\"\"\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "   \n",
    "    shap_selector = ShapRFECV(model, step=0.2, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "   \n",
    "    shap_selector.fit(X, y)\n",
    "\n",
    "   \n",
    "    selected_features = shap_selector.get_reduced_features_set(num_features=n_features)\n",
    "\n",
    "    print(f\"Выбрано {len(selected_features)} признаков: {selected_features}\")\n",
    "    \n",
    "    return X[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_and_select_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Обучает CatBoost, LightGBM, XGBoost и выбирает лучшую модель.\n",
    "    \n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"catboost\": CatBoostClassifier(verbose=0, random_state=42),\n",
    "        \"lightgbm\": LGBMClassifier(random_state=42),\n",
    "        \"xgboost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        results[name] = acc\n",
    "        print(f\"{name} accuracy: {acc}\")\n",
    "    \n",
    "    best_model_name = max(results, key=results.get)\n",
    "    best_model = models[best_model_name]\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def optimize_model_with_optuna(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Настраивает гиперпараметры модели.\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        if model_name == \"catboost\":\n",
    "            params = {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "                'depth': trial.suggest_int('depth', 4, 10),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10)\n",
    "            }\n",
    "            model = CatBoostClassifier(**params, verbose=0, random_state=42)\n",
    "        \n",
    "        elif model_name == \"lightgbm\":\n",
    "            params = {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "            }\n",
    "            model = LGBMClassifier(**params, random_state=42)\n",
    "        \n",
    "        elif model_name == \"xgboost\":\n",
    "            params = {\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 300)\n",
    "            }\n",
    "            model = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "        \n",
    "        score = cross_val_score(model, X, y, cv=3, scoring='accuracy').mean()\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    return study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "\n",
    "class BlendingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _, model in self.models:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.array([model.predict(X) for _, model in self.models])\n",
    "        return np.round(np.mean(preds, axis=0)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Оценивает модель и возвращает результаты в DataFrame.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    results = X_test.copy()\n",
    "    results['y_real'] = y_test\n",
    "    results['y_pred'] = y_pred\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_preprocess_data('/home/vozzy/ml_project/structured_customer_data.csv', drop_columns=['customer_id'], date_column='registration_date')\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбрано 5 признаков: ['average_purchase_value', 'visit_frequency', 'days_since_last_visit', 'customer_feedback_score', 'day']\n"
     ]
    }
   ],
   "source": [
    "X_train_selected = feature_selection(X_train, y_train, n_features=5)\n",
    "X_test_selected = X_test[X_train_selected.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catboost accuracy: 0.915\n",
      "[LightGBM] [Info] Number of positive: 723, number of negative: 77\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 384\n",
      "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.903750 -> initscore=2.239604\n",
      "[LightGBM] [Info] Start training from score 2.239604\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "lightgbm accuracy: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vozzy/ml_project/venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [06:26:42] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost accuracy: 0.91\n",
      "Best model: catboost\n"
     ]
    }
   ],
   "source": [
    "best_model = train_and_select_model(X_train_selected, X_test_selected, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-25 06:27:20,924] A new study created in memory with name: no-name-7b253fab-bcf0-4435-bd7e-8208b96a9b16\n",
      "[I 2024-12-25 06:27:25,264] Trial 0 finished with value: 0.9075028394957432 and parameters: {'learning_rate': 0.00265743718170487, 'depth': 6, 'l2_leaf_reg': 9.206870453952455}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:27:29,243] Trial 1 finished with value: 0.8937606187754029 and parameters: {'learning_rate': 0.010712322058312247, 'depth': 6, 'l2_leaf_reg': 6.137794923428995}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:27:38,174] Trial 2 finished with value: 0.8975012437460693 and parameters: {'learning_rate': 0.007071389361918535, 'depth': 7, 'l2_leaf_reg': 3.8203974758890684}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:27:45,697] Trial 3 finished with value: 0.8875090347591826 and parameters: {'learning_rate': 0.01592266531881893, 'depth': 8, 'l2_leaf_reg': 4.4795948256615485}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:27:53,074] Trial 4 finished with value: 0.9012559488609163 and parameters: {'learning_rate': 0.004039242065594793, 'depth': 8, 'l2_leaf_reg': 9.056050015342825}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:28:00,262] Trial 5 finished with value: 0.8925027925619292 and parameters: {'learning_rate': 0.016077258672749284, 'depth': 8, 'l2_leaf_reg': 5.992567443590161}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:28:22,643] Trial 6 finished with value: 0.8874996479963954 and parameters: {'learning_rate': 0.01573045728345931, 'depth': 10, 'l2_leaf_reg': 2.7590320373338826}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:28:37,181] Trial 7 finished with value: 0.8987543765781494 and parameters: {'learning_rate': 0.007785554142387554, 'depth': 10, 'l2_leaf_reg': 9.403796908796057}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:28:42,400] Trial 8 finished with value: 0.8925027925619292 and parameters: {'learning_rate': 0.012570027159631146, 'depth': 7, 'l2_leaf_reg': 4.362461727994578}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:28:49,507] Trial 9 finished with value: 0.8949902847005152 and parameters: {'learning_rate': 0.0316762923374818, 'depth': 5, 'l2_leaf_reg': 2.6701737361489943}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:28:53,050] Trial 10 finished with value: 0.9050012672129762 and parameters: {'learning_rate': 0.001308683486307678, 'depth': 4, 'l2_leaf_reg': 7.709855887242734}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:28:56,303] Trial 11 finished with value: 0.9037528277622896 and parameters: {'learning_rate': 0.001143036268332906, 'depth': 4, 'l2_leaf_reg': 7.700492188625862}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:28:59,497] Trial 12 finished with value: 0.9037528277622896 and parameters: {'learning_rate': 0.001021371376227767, 'depth': 4, 'l2_leaf_reg': 7.967865152560433}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:29:02,759] Trial 13 finished with value: 0.9074981461143495 and parameters: {'learning_rate': 0.002592794177715945, 'depth': 5, 'l2_leaf_reg': 7.470485764533901}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:29:07,056] Trial 14 finished with value: 0.9075028394957432 and parameters: {'learning_rate': 0.002861279960910941, 'depth': 6, 'l2_leaf_reg': 9.933358340137305}. Best is trial 0 with value: 0.9075028394957432.\n",
      "[I 2024-12-25 06:29:11,729] Trial 15 finished with value: 0.9087512789464297 and parameters: {'learning_rate': 0.0028027559348039266, 'depth': 6, 'l2_leaf_reg': 9.853893935467484}. Best is trial 15 with value: 0.9087512789464297.\n",
      "[I 2024-12-25 06:29:16,041] Trial 16 finished with value: 0.9000075094102297 and parameters: {'learning_rate': 0.00414508061553459, 'depth': 6, 'l2_leaf_reg': 8.646609993103183}. Best is trial 15 with value: 0.9087512789464297.\n",
      "[I 2024-12-25 06:29:23,197] Trial 17 finished with value: 0.8899965268977686 and parameters: {'learning_rate': 0.07080033541481763, 'depth': 5, 'l2_leaf_reg': 6.691070607123743}. Best is trial 15 with value: 0.9087512789464297.\n",
      "[I 2024-12-25 06:29:33,801] Trial 18 finished with value: 0.9050059605943698 and parameters: {'learning_rate': 0.0019184556474852936, 'depth': 9, 'l2_leaf_reg': 9.888956812541078}. Best is trial 15 with value: 0.9087512789464297.\n",
      "[I 2024-12-25 06:29:38,360] Trial 19 finished with value: 0.8987543765781494 and parameters: {'learning_rate': 0.00515208608941777, 'depth': 6, 'l2_leaf_reg': 8.513219012730856}. Best is trial 15 with value: 0.9087512789464297.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.0028027559348039266, 'depth': 6, 'l2_leaf_reg': 9.853893935467484}\n"
     ]
    }
   ],
   "source": [
    "params = optimize_model_with_optuna(X_train_selected, y_train, model_name='catboost')\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "models = [(\"catboost\", CatBoostClassifier(**params, verbose=0))]\n",
    "blender = BlendingClassifier(models)\n",
    "blender.fit(X_train_selected, y_train)\n",
    "results = evaluate_model(blender, X_train_selected, y_train, X_test_selected, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
